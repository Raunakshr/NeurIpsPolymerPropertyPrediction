{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12235747,"sourceType":"datasetVersion","datasetId":7709500},{"sourceId":12330396,"sourceType":"datasetVersion","datasetId":7709869}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/raunakshrestha007/ppp-rdkit-preprocessing-neuroips?scriptVersionId=254776145\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:30:04.174435Z","iopub.execute_input":"2025-08-07T08:30:04.174768Z","iopub.status.idle":"2025-08-07T08:30:04.51037Z","shell.execute_reply.started":"2025-08-07T08:30:04.174743Z","shell.execute_reply":"2025-08-07T08:30:04.509501Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\n/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\n/kaggle/input/tc-smiles/Tc_SMILES.csv\n/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\n/kaggle/input/smiles-extra-data/data_dnst1.xlsx\n/kaggle/input/smiles-extra-data/data_tg3.xlsx\n/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\n/kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:30:04.511751Z","iopub.execute_input":"2025-08-07T08:30:04.512383Z","iopub.status.idle":"2025-08-07T08:30:12.032475Z","shell.execute_reply.started":"2025-08-07T08:30:04.512351Z","shell.execute_reply":"2025-08-07T08:30:12.031656Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nInstalling collected packages: rdkit\nSuccessfully installed rdkit-2025.3.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Data Loading and Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# === Imports ===\nimport pandas as pd\nimport numpy as np\nfrom rdkit import Chem\n\n# === Config ===\nBASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\nTARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\nBAD_PATTERNS = ['[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]',\n                \"[R']\", '[R\"]', 'R1', 'R2', 'R3', 'R4', 'R5',\n                '([R])', '([R1])', '([R2])']\n\n# === SMILES Cleaner ===\ndef clean_and_validate_smiles(smiles):\n    if not isinstance(smiles, str) or not smiles:\n        return None\n    for pattern in BAD_PATTERNS:\n        if pattern in smiles:\n            return None\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            return Chem.MolToSmiles(mol, canonical=True)\n    except:\n        return None\n    return None\n\n# === Load Train/Test ===\ntrain = pd.read_csv(BASE_PATH + 'train.csv')\ntest = pd.read_csv(BASE_PATH + 'test.csv')\n\ntrain['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)\ntest['SMILES'] = test['SMILES'].apply(clean_and_validate_smiles)\n\ntrain.dropna(subset=['SMILES'], inplace=True)\ntest.dropna(subset=['SMILES'], inplace=True)\n\n# === Load External Datasets (excluding dataset2) ===\nexternal_datasets = []\n\ndef load_external(path, target, rename_map=None):\n    try:\n        df = pd.read_csv(path)\n        if rename_map:\n            df = df.rename(columns=rename_map)\n        if 'SMILES' in df.columns and target in df.columns:\n            df = df[['SMILES', target]].dropna()\n            external_datasets.append((target, df))\n            print(f\"✅ Loaded {path} ({len(df)} entries for {target})\")\n        else:\n            print(f\"⚠️ Skipped {path}: required columns missing\")\n    except Exception as e:\n        print(f\"⚠️ Failed to load {path}: {e}\")\n\nload_external(BASE_PATH + 'train_supplement/dataset1.csv', 'Tc', rename_map={'TC_mean': 'Tc'})\nload_external(BASE_PATH + 'train_supplement/dataset3.csv', 'Tg')\nload_external(BASE_PATH + 'train_supplement/dataset4.csv', 'FFV')\n\n# === Load Additional External Datasets ===\ntry:\n    extra_data_tg3 = pd.read_excel(\"/kaggle/input/smiles-extra-data/data_tg3.xlsx\")\n    extra_data_dnst1 = pd.read_excel(\"/kaggle/input/smiles-extra-data/data_dnst1.xlsx\")\n    jcim_sup_bigsmiles = pd.read_csv(\"/kaggle/input/smiles-extra-data/JCIM_sup_bigsmiles.csv\")\n    tc_smiles_df = pd.read_csv(\"/kaggle/input/tc-smiles/Tc_SMILES.csv\")\nexcept Exception as e:\n    print(f\"⚠️ Error loading extra data: {e}\")\n\n# Helper to standardize and append\ndef process_and_append_external(df, target, source_name):\n    if 'SMILES' in df.columns and target in df.columns:\n        df = df[['SMILES', target]].copy()\n        df['SMILES'] = df['SMILES'].apply(clean_and_validate_smiles)\n        df = df.dropna(subset=['SMILES'])\n\n        # Ensure the target column is numeric\n        df[target] = pd.to_numeric(df[target], errors='coerce')\n        df = df.dropna(subset=[target])\n\n        df = df.groupby('SMILES', as_index=False)[target].mean()\n        external_datasets.append((target, df))\n        print(f\"✅ Integrated {source_name}: {len(df)} entries for {target}\")\n    else:\n        print(f\"⚠️ Skipped {source_name}: missing columns\")\n\n# Process each extra dataset (with correct column names)\nprocess_and_append_external(extra_data_tg3.rename(columns={\"Tg [K]\": \"Tg\"}), \"Tg\", \"data_tg3.xlsx\")\nprocess_and_append_external(extra_data_dnst1.rename(columns={\"density(g/cm3)\": \"Density\"}), \"Density\", \"data_dnst1.xlsx\")\nprocess_and_append_external(tc_smiles_df.rename(columns={\"TC_mean\": \"Tc\"}), \"Tc\", \"Tc_SMILES.csv\")\n\n# JCIM SMILES only (for future feature engineering)\njcim_smiles_only = jcim_sup_bigsmiles[['SMILES']].dropna()\njcim_smiles_only['SMILES'] = jcim_smiles_only['SMILES'].apply(clean_and_validate_smiles)\njcim_smiles_only = jcim_smiles_only.dropna().drop_duplicates()\nprint(f\"✅ Loaded JCIM SMILES-only dataset: {len(jcim_smiles_only)} unique SMILES (no targets)\")\n\n# === Merge External Data ===\ndef merge_external(train_df, ext_df, target):\n    ext_df['SMILES'] = ext_df['SMILES'].apply(clean_and_validate_smiles)\n    ext_df = ext_df.dropna(subset=['SMILES', target])\n    ext_df = ext_df.groupby('SMILES', as_index=False)[target].mean()\n\n    # Fill missing target values in existing rows\n    existing_smiles = set(train_df['SMILES'])\n    to_fill = ext_df[ext_df['SMILES'].isin(existing_smiles)]\n    for _, row in to_fill.iterrows():\n        mask = (train_df['SMILES'] == row['SMILES']) & (train_df[target].isna())\n        train_df.loc[mask, target] = row[target]\n\n    # Add new rows\n    new_smiles = set(ext_df['SMILES']) - existing_smiles\n    new_rows = ext_df[ext_df['SMILES'].isin(new_smiles)].copy()\n    for col in TARGETS:\n        if col not in new_rows.columns:\n            new_rows[col] = np.nan\n    return pd.concat([train_df, new_rows[['SMILES'] + TARGETS]], ignore_index=True)\n\n# === Apply Merges ===\ntrain_extended = train[['SMILES'] + TARGETS].copy()\nfor target, ext in external_datasets:\n    train_extended = merge_external(train_extended, ext, target)\n\n# === Final Clean-Up ===\ntrain_extended = train_extended.replace([np.inf, -np.inf], np.nan)\ntrain_extended = train_extended.dropna(subset=TARGETS, how='all')\ntrain_extended = train_extended.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n\n# === Summary ===\nprint(\"\\n📊 Final Summary:\")\nprint(f\"Train: {len(train)} | Extended: {len(train_extended)}\")\nfor t in TARGETS:\n    base = train[t].notna().sum()\n    ext = train_extended[t].notna().sum()\n    print(f\"• {t:<8}: {ext} total ({ext - base:+} from supplements)\")\n\nprint(\"\\n✅ Data loading and preprocessing complete.\")\n\nsmiles_list = train_extended['SMILES'].tolist()\n# Clean SMILES column robustly\ntrain_extended['SMILES'] = train_extended['SMILES'].apply(clean_and_validate_smiles)\n# === Final Clean-Up ===\ntrain_extended = train_extended.replace([np.inf, -np.inf], np.nan)\ntrain_extended = train_extended.dropna(subset=TARGETS, how='all')\ntrain_extended = train_extended.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n\n# === Drop constant columns ===\nconstant_cols = [col for col in train_extended.columns if train_extended[col].nunique() == 1]\ntrain_extended.drop(columns=constant_cols, inplace=True)\nprint(f\"Dropped {len(constant_cols)} constant columns from train_extended\")\n\n\ntrain_extended.shape\ntrain_extended\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:30:12.033794Z","iopub.execute_input":"2025-08-07T08:30:12.034138Z","iopub.status.idle":"2025-08-07T08:30:28.435828Z","shell.execute_reply.started":"2025-08-07T08:30:12.034096Z","shell.execute_reply":"2025-08-07T08:30:28.434912Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv (874 entries for Tc)\n✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv (46 entries for Tg)\n✅ Loaded /kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv (862 entries for FFV)\n✅ Integrated data_tg3.xlsx: 499 entries for Tg\n✅ Integrated data_dnst1.xlsx: 778 entries for Density\n✅ Integrated Tc_SMILES.csv: 866 entries for Tc\n✅ Loaded JCIM SMILES-only dataset: 662 unique SMILES (no targets)\n\n📊 Final Summary:\nTrain: 7973 | Extended: 9990\n• Tg      : 1056 total (+545 from supplements)\n• FFV     : 7892 total (+862 from supplements)\n• Tc      : 866 total (+129 from supplements)\n• Density : 1247 total (+634 from supplements)\n• Rg      : 614 total (+0 from supplements)\n\n✅ Data loading and preprocessing complete.\nDropped 0 constant columns from train_extended\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                 SMILES  Tg       FFV  \\\n0                            *CC(*)c1ccccc1C(=O)OCCCCCC NaN  0.374645   \n1     *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5... NaN  0.370410   \n2     *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(... NaN  0.378860   \n3     *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)... NaN  0.387324   \n4     *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N... NaN  0.355470   \n...                                                 ...  ..       ...   \n9985                                c1ccc(-c2ccccn2)nc1 NaN       NaN   \n9986                      c1ccc(-c2nc3cc4ncoc4cc3o2)cc1 NaN       NaN   \n9987                      c1ccc2oc(-c3ccc4ncoc4c3)nc2c1 NaN       NaN   \n9988                                            c1ccsc1 NaN       NaN   \n9989                                  c1csc(-c2cccs2)c1 NaN       NaN   \n\n            Tc  Density  Rg  \n0     0.205667     1.05 NaN  \n1          NaN      NaN NaN  \n2          NaN      NaN NaN  \n3          NaN      NaN NaN  \n4          NaN      NaN NaN  \n...        ...      ...  ..  \n9985       NaN     1.31 NaN  \n9986       NaN     1.43 NaN  \n9987       NaN     1.43 NaN  \n9988       NaN     1.51 NaN  \n9989       NaN     1.51 NaN  \n\n[9990 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SMILES</th>\n      <th>Tg</th>\n      <th>FFV</th>\n      <th>Tc</th>\n      <th>Density</th>\n      <th>Rg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n      <td>NaN</td>\n      <td>0.374645</td>\n      <td>0.205667</td>\n      <td>1.05</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n      <td>NaN</td>\n      <td>0.370410</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n      <td>NaN</td>\n      <td>0.378860</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n      <td>NaN</td>\n      <td>0.387324</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n      <td>NaN</td>\n      <td>0.355470</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9985</th>\n      <td>c1ccc(-c2ccccn2)nc1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.31</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9986</th>\n      <td>c1ccc(-c2nc3cc4ncoc4cc3o2)cc1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.43</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9987</th>\n      <td>c1ccc2oc(-c3ccc4ncoc4c3)nc2c1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.43</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9988</th>\n      <td>c1ccsc1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.51</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9989</th>\n      <td>c1csc(-c2cccs2)c1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.51</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>9990 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Preprocessing Each Property Separately","metadata":{}},{"cell_type":"code","source":"from rdkit.Chem import AllChem, Descriptors\nfrom sklearn.preprocessing import MinMaxScaler\nfrom rdkit.Chem import rdDistGeom\nimport random\nfrom rdkit import RDLogger\nRDLogger.DisableLog('rdApp.*')\n\n# === Tg: Glass Transition Temperature Preprocessing ===\ndef preprocess_tg(df):\n    print(f\"[Tg] Starting with {len(df)} unique SMILES\")\n    df = df.drop_duplicates(subset='SMILES').copy()\n    df['mol'] = df['SMILES'].apply(lambda s: Chem.MolFromSmiles(s, sanitize=False))\n\n    # Sanitize molecules but allow wildcard '*' (polymerization points)\n    def is_valid_mol(mol):\n        try:\n            if mol is None:\n                return False\n            Chem.SanitizeMol(mol, catchErrors=True)\n            return True\n        except Exception as e:\n            return False\n\n    df['is_valid'] = df['mol'].apply(is_valid_mol)\n    invalid_count = (~df['is_valid']).sum()\n    print(f\"[Tg] Invalid mols removed (e.g. parsing/sanitization issues): {invalid_count}\")\n    df = df[df['is_valid']].drop(columns='is_valid')\n\n    # Generate 3D conformers\n    def generate_conformers(mol):\n        try:\n            mol = Chem.AddHs(mol)\n            params = AllChem.ETKDGv3()\n            params.randomSeed = 42\n            ids = AllChem.EmbedMultipleConfs(mol, numConfs=3, params=params)\n            return ids if ids else []\n        except:\n            return []\n\n    df['conformers'] = df['mol'].apply(generate_conformers)\n    no_conf = (df['conformers'].apply(len) == 0).sum()\n    print(f\"[Tg] Molecules with 0 conformers: {no_conf}\")\n    df = df[df['conformers'].apply(len) > 0]\n\n    print(f\"[Tg] Final Tg samples: {len(df)}\")\n    return df\n\n\n# === FFV: Fractional Free Volume Preprocessing ===\ndef preprocess_ffv(df):\n    df = df.copy()\n    # Remove invalid structures and extreme outliers\n    df = df[df['FFV'].between(0.0, 1.0)]  # plausible physical bounds\n    return df\n\n# === Tc: Thermal Conductivity Preprocessing ===\ndef preprocess_tc(df):\n    df = df.copy()\n    # Remove noise: remove outliers using IQR\n    q1 = df['Tc'].quantile(0.25)\n    q3 = df['Tc'].quantile(0.75)\n    iqr = q3 - q1\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n    df = df[df['Tc'].between(lower, upper)]\n\n    # Data augmentation (e.g., duplicate underrepresented values)\n    # Here: oversample rare Tc ranges (<0.1, >1.5)\n    low_samples = df[df['Tc'] < 0.1]\n    high_samples = df[df['Tc'] > 1.5]\n    df = pd.concat([df, low_samples, high_samples], ignore_index=True)\n\n    # Normalize with MinMax\n    scaler = MinMaxScaler()\n    df['Tc_scaled'] = scaler.fit_transform(df[['Tc']])\n    return df\n\n# === Density: Polymer Density Preprocessing ===\ndef preprocess_density(df, cap_per_bin=300):\n    print(f\"[Density] Starting with {len(df)} samples\")\n    df = df.copy()\n\n    # Step 1: Filter valid physical density range\n    df = df[df['Density'].between(0.5, 2.0)]\n    print(f\"[Density] After bounds filter (0.5–2.0): {len(df)} samples\")\n\n    # Step 2: Normalize SMILES (canonicalize & validate)\n    def normalize_smiles(smi):\n        try:\n            mol = Chem.MolFromSmiles(smi)\n            if mol is None:\n                return None\n            return Chem.MolToSmiles(mol, canonical=True)\n        except:\n            return None\n\n    df['SMILES_norm'] = df['SMILES'].apply(normalize_smiles)\n    num_invalid = df['SMILES_norm'].isnull().sum()\n    print(f\"[Density] Invalid SMILES removed during normalization: {num_invalid}\")\n\n    df = df[df['SMILES_norm'].notnull()].copy()\n    df['SMILES'] = df['SMILES_norm']\n    df = df.drop(columns='SMILES_norm')\n\n    # Step 3: Bin densities\n    df['density_bin'] = pd.cut(df['Density'], bins=[0.5, 1.0, 1.5, 2.0])\n    bin_counts = df['density_bin'].value_counts().sort_index()\n    print(\"[Density] Bin counts before capping:\")\n    print(bin_counts)\n\n    # Step 4: Cap each bin to avoid large imbalance\n    def cap_bin(group):\n        return group.sample(min(len(group), cap_per_bin), random_state=42)\n\n    df = (\n        df.groupby('density_bin', observed=False)\n        .apply(cap_bin)\n        .reset_index(drop=True)\n    )\n\n    print(f\"[Density] Final capped samples: {len(df)}\")\n    return df\n\n\n# === Rg: Radius of Gyration Preprocessing ===\ndef preprocess_rg(df):\n    from rdkit import Chem\n    from rdkit.Chem import AllChem\n\n    df = df.copy()\n\n    def smiles_to_3d_polymer_safe(smiles):\n        try:\n            # Parse SMILES without sanitizing (to allow wildcard *)\n            mol = Chem.MolFromSmiles(smiles, sanitize=False)\n            if mol is None:\n                return None\n\n            # Manually update valence information (avoids implicit Hs error)\n            for atom in mol.GetAtoms():\n                atom.UpdatePropertyCache(strict=False)\n\n            # Add explicit hydrogens\n            mol = Chem.AddHs(mol)\n\n            # Generate 3D conformer\n            params = AllChem.ETKDGv3()\n            params.randomSeed = 42\n            success = AllChem.EmbedMolecule(mol, params)\n            if success != 0:\n                return None\n\n            # Optimize geometry\n            AllChem.UFFOptimizeMolecule(mol)\n\n            return mol\n\n        except Exception as e:\n            return None\n\n    # Apply 3D generation to Rg SMILES\n    df['mol_3d'] = df['SMILES'].apply(smiles_to_3d_polymer_safe)\n\n    # Drop failed conversions\n    df = df[df['mol_3d'].notnull()].reset_index(drop=True)\n\n    return df\n\n\n\n# === Apply All Preprocessing Steps ===\ntg_df = preprocess_tg(train_extended[train_extended['Tg'].notna()])\nffv_df = preprocess_ffv(train_extended[train_extended['FFV'].notna()])\ntc_df = preprocess_tc(train_extended[train_extended['Tc'].notna()])\ndensity_df = preprocess_density(train_extended[train_extended['Density'].notna()], cap_per_bin=300)\nrg_df = preprocess_rg(train_extended[train_extended['Rg'].notna()])\n\nprint(\"✅ All property-specific preprocessing complete.\")\nprint(f\"Tg samples: {len(tg_df)}\")\nprint(f\"FFV samples: {len(ffv_df)}\")\nprint(f\"Tc samples: {len(tc_df)}\")\nprint(f\"Density samples: {len(density_df)}\")\nprint(f\"Rg samples: {len(rg_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:30:28.437861Z","iopub.execute_input":"2025-08-07T08:30:28.43846Z","iopub.status.idle":"2025-08-07T08:35:24.799101Z","shell.execute_reply.started":"2025-08-07T08:30:28.438427Z","shell.execute_reply":"2025-08-07T08:35:24.798289Z"}},"outputs":[{"name":"stdout","text":"[Tg] Starting with 1056 unique SMILES\n[Tg] Invalid mols removed (e.g. parsing/sanitization issues): 0\n[Tg] Molecules with 0 conformers: 15\n[Tg] Final Tg samples: 1041\n[Density] Starting with 1247 samples\n[Density] After bounds filter (0.5–2.0): 1246 samples\n[Density] Invalid SMILES removed during normalization: 0\n[Density] Bin counts before capping:\ndensity_bin\n(0.5, 1.0]    456\n(1.0, 1.5]    673\n(1.5, 2.0]    117\nName: count, dtype: int64\n[Density] Final capped samples: 717\n✅ All property-specific preprocessing complete.\nTg samples: 1041\nFFV samples: 7892\nTc samples: 879\nDensity samples: 717\nRg samples: 597\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Feature Engineering Pipeline","metadata":{}},{"cell_type":"code","source":"from rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors, AllChem\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# === Tg Feature Engineering ===\ndef featurize_tg(df):\n    df = df.copy()\n    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n\n    # Morgan fingerprints (radius=2, nBits=1024)\n    df['MorganFP'] = df['mol'].apply(lambda m: AllChem.GetMorganFingerprintAsBitVect(m, radius=2, nBits=1024) if m else None)\n\n    # Physico-chemical descriptors\n    desc_funcs = {\n        'MolWt': Descriptors.MolWt,\n        'NumHDonors': Descriptors.NumHDonors,\n        'NumHAcceptors': Descriptors.NumHAcceptors,\n        'TPSA': rdMolDescriptors.CalcTPSA,\n        'MolLogP': Descriptors.MolLogP,\n        'NumRotatableBonds': Descriptors.NumRotatableBonds,\n    }\n    \n    feats = {name: [] for name in desc_funcs}\n    for m in tqdm(df['mol'], desc='Featurizing Tg'):\n        if m:\n            for name, func in desc_funcs.items():\n                feats[name].append(func(m))\n        else:\n            for name in desc_funcs:\n                feats[name].append(np.nan)\n    \n    feats_df = pd.DataFrame(feats, index=df.index)\n    df = pd.concat([df, feats_df], axis=1)\n\n    return df\n\n# === FFV Feature Engineering ===\ndef featurize_ffv(df):\n    df = df.copy()\n    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n\n    feats = {\n        'MolWt': [],\n        'TPSA': [],\n        'LabuteASA': [],\n        'PEOE_VSA1': [],\n    }\n    for m in tqdm(df['mol'], desc='Featurizing FFV'):\n        if m:\n            feats['MolWt'].append(Descriptors.MolWt(m))\n            feats['TPSA'].append(rdMolDescriptors.CalcTPSA(m))\n            feats['LabuteASA'].append(rdMolDescriptors.CalcLabuteASA(m))\n            feats['PEOE_VSA1'].append(Descriptors.PEOE_VSA1(m)) \n        else:\n            for k in feats:\n                feats[k].append(np.nan)\n\n    feats_df = pd.DataFrame(feats, index=df.index)\n    df = pd.concat([df, feats_df], axis=1)\n    return df\n\n# === Tc Feature Engineering ===\ndef featurize_tc(df):\n    df = df.copy()\n    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n    \n    feats = {\n        'MolSurfaceArea': [],\n        'MolVolume': [],\n        'NumAtoms': [],\n        'InteratomicDistancesMean': [],\n    }\n\n    for m in tqdm(df['mol'], desc='Featurizing Tc'):\n        if m:\n            # Molecular surface area & volume using Crippen descriptors as proxy\n            feats['MolSurfaceArea'].append(Descriptors.MolMR(m))\n            feats['MolVolume'].append(Descriptors.MolLogP(m))  # substitute for volume, or replaceww with 3D volume if available\n            feats['NumAtoms'].append(m.GetNumAtoms())\n            \n            # Mean pairwise interatomic distances (from 3D conf if present)\n            try:\n                mol3d = Chem.AddHs(m)\n                AllChem.EmbedMolecule(mol3d)\n                conf = mol3d.GetConformer()\n                dists = []\n                n_atoms = mol3d.GetNumAtoms()\n                for i in range(n_atoms):\n                    pos_i = conf.GetAtomPosition(i)\n                    for j in range(i+1, n_atoms):\n                        pos_j = conf.GetAtomPosition(j)\n                        dist = pos_i.Distance(pos_j)\n                        dists.append(dist)\n                feats['InteratomicDistancesMean'].append(np.mean(dists) if dists else np.nan)\n            except:\n                feats['InteratomicDistancesMean'].append(np.nan)\n        else:\n            for k in feats:\n                feats[k].append(np.nan)\n    \n    feats_df = pd.DataFrame(feats, index=df.index)\n    df = pd.concat([df, feats_df], axis=1)\n    return df\n\n# === Density Feature Engineering ===\ndef featurize_density(df):\n    df = df.copy()\n    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n    \n    feats = {\n        'MolVolume3D': [],\n        'MolSurfaceArea3D': [],\n        'Density_MD': [],\n    }\n\n    for m in tqdm(df['mol'], desc='Featurizing Density'):\n        if m:\n            try:\n                mol3d = Chem.AddHs(m)\n                AllChem.EmbedMolecule(mol3d, randomSeed=42)\n                AllChem.UFFOptimizeMolecule(mol3d)\n                vol = AllChem.ComputeMolVolume(mol3d)\n                sa = AllChem.ComputeMolSurfaceArea(mol3d)\n                feats['MolVolume3D'].append(vol)\n                feats['MolSurfaceArea3D'].append(sa)\n                feats['Density_MD'].append(np.nan)  # replace with MD density if available\n            except:\n                feats['MolVolume3D'].append(np.nan)\n                feats['MolSurfaceArea3D'].append(np.nan)\n                feats['Density_MD'].append(np.nan)\n        else:\n            for k in feats:\n                feats[k].append(np.nan)\n    \n    feats_df = pd.DataFrame(feats, index=df.index)\n    df = pd.concat([df, feats_df], axis=1)\n    return df\n\n# === Rg Feature Engineering ===\ndef featurize_rg(df):\n    df = df.copy()\n    df['mol_3d'] = df['SMILES'].apply(lambda smi: Chem.MolFromSmiles(smi))\n    \n    feats = {\n        'Radius': [],\n        'MomentOfInertia': [],\n        'ConformerCoordsMeanX': [],\n        'ConformerCoordsMeanY': [],\n        'ConformerCoordsMeanZ': [],\n    }\n    \n    for m in tqdm(df['mol_3d'], desc='Featurizing Rg'):\n        if m:\n            try:\n                mol3d = Chem.AddHs(m)\n                AllChem.EmbedMolecule(mol3d, randomSeed=42)\n                AllChem.UFFOptimizeMolecule(mol3d)\n                conf = mol3d.GetConformer()\n                n_atoms = mol3d.GetNumAtoms()\n                coords = np.array([list(conf.GetAtomPosition(i)) for i in range(n_atoms)])\n                \n                center = coords.mean(axis=0)\n                dists = np.linalg.norm(coords - center, axis=1)\n                radius = dists.max()\n                \n                mass = np.array([atom.GetMass() for atom in mol3d.GetAtoms()])\n                rel_coords = coords - center\n                moi = np.sum(mass * np.sum(rel_coords**2, axis=1))\n                \n                feats['Radius'].append(radius)\n                feats['MomentOfInertia'].append(moi)\n                feats['ConformerCoordsMeanX'].append(center[0])\n                feats['ConformerCoordsMeanY'].append(center[1])\n                feats['ConformerCoordsMeanZ'].append(center[2])\n            except:\n                for k in feats:\n                    feats[k].append(np.nan)\n        else:\n            for k in feats:\n                feats[k].append(np.nan)\n    \n    feats_df = pd.DataFrame(feats, index=df.index)\n    df = pd.concat([df, feats_df], axis=1)\n    return df\n\n\n# === Example usage ===\ntg_features = featurize_tg(tg_df)\nffv_features = featurize_ffv(ffv_df)\ntc_features = featurize_tc(tc_df)\ndensity_features = featurize_density(density_df)\nrg_features = featurize_rg(rg_df)\n\nprint(\"Feature engineering complete.\")\nprint(f\"Tg features shape: {tg_features.shape}\")\nprint(f\"FFV features shape: {ffv_features.shape}\")\nprint(f\"Tc features shape: {tc_features.shape}\")\nprint(f\"Density features shape: {density_features.shape}\")\nprint(f\"Rg features shape: {rg_features.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T08:35:24.800049Z","iopub.execute_input":"2025-08-07T08:35:24.800524Z","iopub.status.idle":"2025-08-07T08:40:01.38521Z","shell.execute_reply.started":"2025-08-07T08:35:24.800501Z","shell.execute_reply":"2025-08-07T08:40:01.384272Z"}},"outputs":[{"name":"stderr","text":"Featurizing Tg: 100%|██████████| 1041/1041 [00:00<00:00, 1899.91it/s]\nFeaturizing FFV: 100%|██████████| 7892/7892 [00:00<00:00, 12737.28it/s]\nFeaturizing Tc: 100%|██████████| 879/879 [01:03<00:00, 13.90it/s]\nFeaturizing Density: 100%|██████████| 717/717 [02:17<00:00,  5.20it/s]\nFeaturizing Rg: 100%|██████████| 597/597 [01:10<00:00,  8.52it/s]","output_type":"stream"},{"name":"stdout","text":"Feature engineering complete.\nTg features shape: (1041, 15)\nFFV features shape: (7892, 11)\nTc features shape: (879, 12)\nDensity features shape: (717, 11)\nRg features shape: (597, 12)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Full Install Command (PyTorch 2.1.0 + CPU)","metadata":{}},{"cell_type":"code","source":"!pip install -q torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0\n!pip install -q rdkit-pypi\n\n# Install PyTorch Geometric and dependencies\n!pip install -q torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cpu.html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T11:42:08.212677Z","iopub.execute_input":"2025-08-07T11:42:08.213067Z","iopub.status.idle":"2025-08-07T11:45:40.528723Z","shell.execute_reply.started":"2025-08-07T11:42:08.21302Z","shell.execute_reply":"2025-08-07T11:45:40.526898Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m105.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Required Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom rdkit import Chem\nfrom rdkit.Chem import rdmolops\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm\nimport pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing: Convert SMILES to Graph","metadata":{}},{"cell_type":"code","source":"# Atom-level features\ndef atom_features(atom):\n    return torch.tensor([\n        atom.GetAtomicNum(),           \n        atom.GetDegree(),              \n        atom.GetImplicitValence(),     \n        atom.GetFormalCharge(),        \n        float(atom.GetHybridization().real)\n    ], dtype=torch.float)\n\n# SMILES to PyG graph\ndef mol_to_graph(smiles, label):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n\n    x = torch.stack([atom_features(atom) for atom in mol.GetAtoms()])\n    \n    edge_index = []\n    for bond in mol.GetBonds():\n        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n        edge_index += [[start, end], [end, start]]\n\n    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n    y = torch.tensor([label], dtype=torch.float)\n\n    return Data(x=x, edge_index=edge_index, y=y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"# Example: df = pd.read_csv(\"your_dataset.csv\")\n# df.head()\n\ngraph_data = []\nfor i, row in tqdm(df.iterrows(), total=len(df)):\n    graph = mol_to_graph(row['SMILES'], row['FFV'])  # <- Change 'FFV' to your target if needed\n    if graph is not None:\n        graph_data.append(graph)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train/Test Split + DataLoaders","metadata":{}},{"cell_type":"code","source":"train_idx, test_idx = train_test_split(range(len(graph_data)), test_size=0.2, random_state=42)\n\ntrain_data = [graph_data[i] for i in train_idx]\ntest_data = [graph_data[i] for i in test_idx]\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GCN Model Definition","metadata":{}},{"cell_type":"code","source":"class GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index, batch):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = global_mean_pool(x, batch)\n        x = self.lin(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = GCN(\n    in_channels=train_data[0].x.shape[1],\n    hidden_channels=64,\n    out_channels=1\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.MSELoss()\n\n# Train\nfor epoch in range(1, 51):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch.x, batch.edge_index, batch.batch).view(-1)\n        loss = loss_fn(out, batch.y.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n    print(f\"Epoch {epoch:02d} | Loss: {total_loss / len(train_loader.dataset):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation (R² Score)","metadata":{}},{"cell_type":"code","source":"model.eval()\npreds, trues = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        out = model(batch.x, batch.edge_index, batch.batch).view(-1).cpu()\n        preds.append(out)\n        trues.append(batch.y.view(-1).cpu())\n\npreds = torch.cat(preds).numpy()\ntrues = torch.cat(trues).numpy()\n\nprint(\"R² Score:\", r2_score(trues, preds))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}